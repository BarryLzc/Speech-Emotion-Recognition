{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb2023df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import noisereduce\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#import torchvision\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import rcParams#new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71cc4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_df = pd.read_csv('Summary_Emo_Eval.csv')\n",
    "\n",
    "lab_dic = {}\n",
    "for ind, row in lab_df.iterrows():\n",
    "    label = row['emotion']\n",
    "    if label == 'xxx':\n",
    "        lab_dic[row['wav_filename']] = 'oth' # other\n",
    "    elif label == 'exc': # excited\n",
    "        lab_dic[row['wav_filename']] = 'hap' # happy\n",
    "    else:\n",
    "        lab_dic[row['wav_filename']] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53cbea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'IEMOCAP/'\n",
    "\n",
    "Y = []\n",
    "mfcc_list = []\n",
    "\n",
    "for session in range(1,6):#range(1,6)\n",
    "    wav_sess_dir = dataset_dir + f'Session{session}/sentences/wav/'\n",
    "    dialog_names = os.listdir(wav_sess_dir)\n",
    "    for dialog in dialog_names:\n",
    "        if dialog.startswith('.'):\n",
    "            continue\n",
    "        wav_dialog_dir = wav_sess_dir+dialog+'/'\n",
    "        sentences_names = os.listdir(wav_dialog_dir)\n",
    "        for sentence in sentences_names:\n",
    "            if sentence.startswith('.'):\n",
    "                continue\n",
    "            if not sentence.endswith('wav'):\n",
    "                continue\n",
    "            label = [lab_dic[sentence[:-4]]]\n",
    "            if not label[0] in ['ang','hap','sad','neu']:\n",
    "                continue\n",
    "            wav_sentence_path = wav_dialog_dir+sentence\n",
    "            waveform, sr = librosa.load(wav_sentence_path, sr=None)\n",
    "            mfcc = np.mean(librosa.feature.mfcc(y=waveform, sr=sr, n_mfcc=40).T, axis=0)\n",
    "            Y.append(label)\n",
    "            mfcc_list.append(mfcc.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7d2e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=mfcc_list\n",
    "X_all = np.array(X)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit([['ang'],['hap'],['sad'],['neu']])\n",
    "Y_all = enc.transform(Y).toarray()\n",
    "\n",
    "Xtrain, Xvaltest, Ytrain, Yvaltest = train_test_split(X_all, Y_all, test_size=0.2, random_state=42, shuffle=True)\n",
    "Xval, Xtest, Yval, Ytest = train_test_split(Xvaltest, Yvaltest, test_size=0.5, random_state=42, shuffle=True)\n",
    "\n",
    "Xtrain_c = np.expand_dims(Xtrain, axis=1)\n",
    "Xval_c = np.expand_dims(Xval, axis=1)\n",
    "Xtest_c = np.expand_dims(Xtest, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bc9a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 256, 5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(256, 128, 5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.conv3 = nn.Conv1d(128, 128, 5, padding=2)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.pool3 = nn.MaxPool1d(2)\n",
    "        self.conv4 = nn.Conv1d(128, 128, 5, padding=2)\n",
    "        self.conv5 = nn.Conv1d(128, 128, 5, padding=2)\n",
    "        self.bn4 = nn.BatchNorm1d(128)\n",
    "        self.conv6 = nn.Conv1d(128, 128, 5, padding=2)\n",
    "        self.fc = nn.Linear(640, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "net = NNModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f113326",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_ts  = torch.Tensor(Xtrain_c)\n",
    "Ytrain_ts  = torch.Tensor(Ytrain)\n",
    "\n",
    "Xval_ts = torch.Tensor(Xval_c)\n",
    "Yval_ts = torch.Tensor(Yval)\n",
    "\n",
    "Xtest_ts = torch.Tensor(Xtest_c)\n",
    "Ytest_ts = torch.Tensor(Ytest)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_set = torch.utils.data.TensorDataset(Xtrain_ts,Ytrain_ts)\n",
    "train_loader = torch.utils.data.DataLoader(train_set,batch_size=batch_size,shuffle=False,num_workers=2)\n",
    "\n",
    "val_set = torch.utils.data.TensorDataset(Xval_ts,Yval_ts)\n",
    "val_loader = torch.utils.data.DataLoader(val_set,batch_size=batch_size,shuffle=False,num_workers=2)\n",
    "\n",
    "test_set = torch.utils.data.TensorDataset(Xtest_ts,Ytest_ts)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,batch_size=batch_size,shuffle=False,num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f116239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "def Tlabels(a):\n",
    "    L1=[]\n",
    "    for x in a:\n",
    "        L1.append(x)\n",
    "    return L1\n",
    "\n",
    "\n",
    "def Plabels(b):\n",
    "    L2=[]\n",
    "    for x in b:\n",
    "        L2.append(x)\n",
    "    return L2\n",
    "\n",
    "def conf_mat(cm,L1,L2):\n",
    "    if len(L2)<1:\n",
    "        return cm\n",
    "    for i in range(len(L1)):\n",
    "        if L2[i]==L1[i]:\n",
    "            cm[L1[i]][L1[i]]=cm[L1[i]][L1[i]]+1\n",
    "        else:\n",
    "            cm[L2[i]][L1[i]]=cm[L2[i]][L1[i]]+1\n",
    "    return cm\n",
    "            \n",
    "def cmplot(cm):\n",
    "    classes = ['ang','hap','sad','neu']\n",
    "    confusion_matrix = np.array(cm,dtype=np.int)\n",
    "    proportion=[]\n",
    "    for i in confusion_matrix:\n",
    "        for j in i:\n",
    "            temp=j/(np.sum(i))\n",
    "            proportion.append(temp)\n",
    "    pshow=[]\n",
    "    for i in proportion:\n",
    "        pt=\"%.2f%%\" % (i * 100)\n",
    "        pshow.append(pt)\n",
    "    proportion=np.array(proportion).reshape(4,4)  \n",
    "    pshow=np.array(pshow).reshape(4,4)\n",
    "    #print(pshow)\n",
    "    config = {\n",
    "        \"font.family\":'Times New Roman',  \n",
    "    }\n",
    "    rcParams.update(config)\n",
    "    plt.imshow(proportion, interpolation='nearest', cmap=plt.cm.Blues) \n",
    "    plt.title('confusion_matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes,fontsize=12)\n",
    "    plt.yticks(tick_marks, classes,fontsize=12)\n",
    "    \n",
    "    thresh = confusion_matrix.max() / 2.\n",
    "\n",
    "    iters = np.reshape([[[i,j] for j in range(4)] for i in range(4)],(confusion_matrix.size,2))\n",
    "    for i, j in iters:\n",
    "        if(i==j):\n",
    "            plt.text(j, i - 0.12, format(confusion_matrix[i, j]), va='center', ha='center', fontsize=12,color='white',weight=5) \n",
    "            plt.text(j, i + 0.12, pshow[i, j], va='center', ha='center', fontsize=12,color='white')\n",
    "        else:\n",
    "            plt.text(j, i-0.12, format(confusion_matrix[i, j]),va='center',ha='center',fontsize=12)\n",
    "            plt.text(j, i+0.12, pshow[i, j], va='center', ha='center', fontsize=12)\n",
    "    \n",
    "    plt.ylabel('True label',fontsize=16)\n",
    "    plt.xlabel('Predict label',fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4415d1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     1:\n",
      "Training Loss 0.241; Training Acc 78.549%; Validation Acc 55.515%; Test Acc 55.054%\n",
      "Epoch    10:\n",
      "Training Loss 0.236; Training Acc 80.493%; Validation Acc 56.962%; Test Acc 54.693%\n",
      "Epoch    20:\n",
      "Training Loss 0.231; Training Acc 82.324%; Validation Acc 58.590%; Test Acc 56.679%\n",
      "Epoch    30:\n",
      "Training Loss 0.227; Training Acc 83.929%; Validation Acc 56.420%; Test Acc 58.845%\n",
      "Epoch    40:\n",
      "Training Loss 0.224; Training Acc 85.420%; Validation Acc 59.494%; Test Acc 56.679%\n",
      "Epoch    50:\n",
      "Training Loss 0.221; Training Acc 86.234%; Validation Acc 56.600%; Test Acc 57.040%\n",
      "Epoch    60:\n",
      "Training Loss 0.218; Training Acc 87.455%; Validation Acc 57.143%; Test Acc 59.025%\n",
      "Epoch    70:\n",
      "Training Loss 0.216; Training Acc 87.997%; Validation Acc 58.590%; Test Acc 57.762%\n",
      "Epoch    80:\n",
      "Training Loss 0.214; Training Acc 89.150%; Validation Acc 55.877%; Test Acc 58.845%\n",
      "Epoch    90:\n",
      "Training Loss 0.212; Training Acc 89.715%; Validation Acc 55.696%; Test Acc 58.303%\n",
      "Epoch   100:\n",
      "Training Loss 0.210; Training Acc 90.393%; Validation Acc 57.143%; Test Acc 57.581%\n",
      "Epoch   110:\n",
      "Training Loss 0.210; Training Acc 90.913%; Validation Acc 56.962%; Test Acc 58.303%\n",
      "Epoch   120:\n",
      "Training Loss 0.208; Training Acc 91.501%; Validation Acc 56.420%; Test Acc 57.942%\n",
      "Epoch   130:\n",
      "Training Loss 0.207; Training Acc 91.998%; Validation Acc 56.781%; Test Acc 56.679%\n",
      "Epoch   140:\n",
      "Training Loss 0.206; Training Acc 92.269%; Validation Acc 56.781%; Test Acc 57.581%\n",
      "Epoch   150:\n",
      "Training Loss 0.205; Training Acc 92.541%; Validation Acc 56.058%; Test Acc 60.108%\n",
      "Epoch   160:\n",
      "Training Loss 0.205; Training Acc 92.676%; Validation Acc 58.409%; Test Acc 60.108%\n",
      "Epoch   170:\n",
      "Training Loss 0.204; Training Acc 92.699%; Validation Acc 56.781%; Test Acc 57.581%\n",
      "Epoch   180:\n",
      "Training Loss 0.203; Training Acc 93.128%; Validation Acc 56.781%; Test Acc 55.596%\n",
      "Epoch   190:\n",
      "Training Loss 0.203; Training Acc 93.400%; Validation Acc 56.600%; Test Acc 59.567%\n",
      "Epoch   200:\n",
      "Training Loss 0.202; Training Acc 93.671%; Validation Acc 56.239%; Test Acc 59.567%\n",
      "Epoch   210:\n",
      "Training Loss 0.202; Training Acc 93.784%; Validation Acc 58.770%; Test Acc 59.206%\n",
      "Epoch   220:\n",
      "Training Loss 0.201; Training Acc 93.807%; Validation Acc 55.877%; Test Acc 57.401%\n",
      "Epoch   230:\n",
      "Training Loss 0.201; Training Acc 93.965%; Validation Acc 55.154%; Test Acc 55.776%\n",
      "Epoch   240:\n",
      "Training Loss 0.201; Training Acc 94.168%; Validation Acc 56.600%; Test Acc 60.108%\n",
      "Epoch   250:\n",
      "Training Loss 0.201; Training Acc 93.965%; Validation Acc 54.430%; Test Acc 61.011%\n",
      "Epoch   260:\n",
      "Training Loss 0.200; Training Acc 94.394%; Validation Acc 56.962%; Test Acc 59.747%\n",
      "Epoch   270:\n",
      "Training Loss 0.200; Training Acc 94.439%; Validation Acc 55.877%; Test Acc 58.664%\n",
      "Epoch   280:\n",
      "Training Loss 0.200; Training Acc 94.552%; Validation Acc 58.047%; Test Acc 58.845%\n",
      "Epoch   290:\n",
      "Training Loss 0.199; Training Acc 94.643%; Validation Acc 55.877%; Test Acc 60.650%\n",
      "Epoch   300:\n",
      "Training Loss 0.199; Training Acc 94.756%; Validation Acc 56.058%; Test Acc 57.040%\n",
      "Epoch   310:\n",
      "Training Loss 0.199; Training Acc 94.869%; Validation Acc 55.154%; Test Acc 59.025%\n",
      "Epoch   320:\n",
      "Training Loss 0.199; Training Acc 94.937%; Validation Acc 55.515%; Test Acc 60.830%\n",
      "Epoch   330:\n",
      "Training Loss 0.199; Training Acc 94.914%; Validation Acc 57.685%; Test Acc 59.567%\n",
      "Epoch   340:\n",
      "Training Loss 0.198; Training Acc 95.208%; Validation Acc 56.058%; Test Acc 59.025%\n",
      "Epoch   350:\n",
      "Training Loss 0.198; Training Acc 95.344%; Validation Acc 59.132%; Test Acc 61.552%\n",
      "Epoch   360:\n",
      "Training Loss 0.198; Training Acc 95.321%; Validation Acc 56.962%; Test Acc 58.484%\n",
      "Epoch   370:\n",
      "Training Loss 0.198; Training Acc 95.434%; Validation Acc 56.781%; Test Acc 57.581%\n",
      "Epoch   380:\n",
      "Training Loss 0.197; Training Acc 95.434%; Validation Acc 56.058%; Test Acc 58.484%\n",
      "Epoch   390:\n",
      "Training Loss 0.197; Training Acc 95.705%; Validation Acc 56.781%; Test Acc 59.025%\n",
      "Epoch   400:\n",
      "Training Loss 0.197; Training Acc 95.750%; Validation Acc 55.154%; Test Acc 58.845%\n",
      "Epoch   410:\n",
      "Training Loss 0.197; Training Acc 95.773%; Validation Acc 54.973%; Test Acc 59.025%\n",
      "Epoch   420:\n",
      "Training Loss 0.196; Training Acc 95.796%; Validation Acc 57.324%; Test Acc 58.484%\n",
      "Epoch   430:\n",
      "Training Loss 0.197; Training Acc 95.728%; Validation Acc 57.866%; Test Acc 58.123%\n",
      "Epoch   440:\n",
      "Training Loss 0.196; Training Acc 95.886%; Validation Acc 58.228%; Test Acc 60.650%\n",
      "Epoch   450:\n",
      "Training Loss 0.196; Training Acc 95.841%; Validation Acc 58.409%; Test Acc 57.942%\n",
      "Epoch   460:\n",
      "Training Loss 0.196; Training Acc 95.976%; Validation Acc 56.600%; Test Acc 59.928%\n",
      "Epoch   470:\n",
      "Training Loss 0.196; Training Acc 95.999%; Validation Acc 58.409%; Test Acc 58.845%\n",
      "Epoch   480:\n",
      "Training Loss 0.196; Training Acc 96.022%; Validation Acc 57.324%; Test Acc 61.191%\n",
      "Epoch   490:\n",
      "Training Loss 0.196; Training Acc 96.044%; Validation Acc 57.685%; Test Acc 58.484%\n",
      "Epoch   500:\n",
      "Training Loss 0.196; Training Acc 96.067%; Validation Acc 57.324%; Test Acc 59.386%\n",
      "Epoch   510:\n",
      "Training Loss 0.196; Training Acc 96.090%; Validation Acc 57.866%; Test Acc 58.484%\n",
      "Epoch   520:\n",
      "Training Loss 0.196; Training Acc 96.044%; Validation Acc 56.962%; Test Acc 59.386%\n",
      "Epoch   530:\n",
      "Training Loss 0.196; Training Acc 96.157%; Validation Acc 57.685%; Test Acc 59.928%\n",
      "Epoch   540:\n",
      "Training Loss 0.195; Training Acc 96.203%; Validation Acc 58.228%; Test Acc 58.664%\n",
      "Epoch   550:\n",
      "Training Loss 0.195; Training Acc 96.157%; Validation Acc 57.866%; Test Acc 59.206%\n",
      "Epoch   560:\n",
      "Training Loss 0.195; Training Acc 96.180%; Validation Acc 59.132%; Test Acc 60.289%\n",
      "Epoch   570:\n",
      "Training Loss 0.195; Training Acc 96.225%; Validation Acc 60.036%; Test Acc 59.747%\n",
      "Epoch   580:\n",
      "Training Loss 0.196; Training Acc 96.157%; Validation Acc 57.866%; Test Acc 59.025%\n",
      "Epoch   590:\n",
      "Training Loss 0.195; Training Acc 96.248%; Validation Acc 58.590%; Test Acc 60.469%\n",
      "Epoch   600:\n",
      "Training Loss 0.195; Training Acc 96.248%; Validation Acc 58.770%; Test Acc 59.747%\n",
      "Epoch   610:\n",
      "Training Loss 0.195; Training Acc 96.225%; Validation Acc 58.228%; Test Acc 59.206%\n",
      "Epoch   620:\n",
      "Training Loss 0.195; Training Acc 96.293%; Validation Acc 56.239%; Test Acc 58.664%\n",
      "Epoch   630:\n",
      "Training Loss 0.195; Training Acc 96.338%; Validation Acc 56.962%; Test Acc 59.747%\n",
      "Epoch   640:\n",
      "Training Loss 0.195; Training Acc 96.406%; Validation Acc 57.143%; Test Acc 60.289%\n",
      "Epoch   650:\n",
      "Training Loss 0.195; Training Acc 96.406%; Validation Acc 58.047%; Test Acc 60.830%\n",
      "Epoch   660:\n",
      "Training Loss 0.195; Training Acc 96.406%; Validation Acc 57.685%; Test Acc 58.664%\n",
      "Epoch   670:\n",
      "Training Loss 0.195; Training Acc 96.429%; Validation Acc 55.696%; Test Acc 60.830%\n",
      "Epoch   680:\n",
      "Training Loss 0.195; Training Acc 96.474%; Validation Acc 57.143%; Test Acc 59.386%\n",
      "Epoch   690:\n",
      "Training Loss 0.195; Training Acc 96.451%; Validation Acc 56.058%; Test Acc 57.581%\n",
      "Epoch   700:\n",
      "Training Loss 0.195; Training Acc 96.496%; Validation Acc 58.047%; Test Acc 59.206%\n",
      "Epoch   710:\n",
      "Training Loss 0.195; Training Acc 96.609%; Validation Acc 55.515%; Test Acc 59.567%\n",
      "Epoch   720:\n",
      "Training Loss 0.194; Training Acc 96.587%; Validation Acc 57.143%; Test Acc 59.206%\n",
      "Epoch   730:\n",
      "Training Loss 0.195; Training Acc 96.542%; Validation Acc 56.781%; Test Acc 60.469%\n",
      "Epoch   740:\n",
      "Training Loss 0.194; Training Acc 96.632%; Validation Acc 58.951%; Test Acc 60.108%\n",
      "Epoch   750:\n",
      "Training Loss 0.194; Training Acc 96.632%; Validation Acc 56.420%; Test Acc 60.289%\n",
      "Epoch   760:\n",
      "Training Loss 0.194; Training Acc 96.632%; Validation Acc 56.962%; Test Acc 59.386%\n",
      "Epoch   770:\n",
      "Training Loss 0.194; Training Acc 96.542%; Validation Acc 56.058%; Test Acc 58.845%\n",
      "Epoch   780:\n",
      "Training Loss 0.194; Training Acc 96.655%; Validation Acc 55.335%; Test Acc 59.025%\n",
      "Epoch   790:\n",
      "Training Loss 0.194; Training Acc 96.564%; Validation Acc 57.685%; Test Acc 58.123%\n",
      "Epoch   800:\n",
      "Training Loss 0.194; Training Acc 96.655%; Validation Acc 58.047%; Test Acc 58.303%\n",
      "Epoch   810:\n",
      "Training Loss 0.194; Training Acc 96.677%; Validation Acc 57.143%; Test Acc 59.386%\n",
      "Epoch   820:\n",
      "Training Loss 0.194; Training Acc 96.677%; Validation Acc 56.058%; Test Acc 59.206%\n",
      "Epoch   830:\n",
      "Training Loss 0.194; Training Acc 96.677%; Validation Acc 58.047%; Test Acc 59.747%\n",
      "Epoch   840:\n",
      "Training Loss 0.194; Training Acc 96.677%; Validation Acc 58.951%; Test Acc 58.123%\n",
      "Epoch   850:\n",
      "Training Loss 0.194; Training Acc 96.700%; Validation Acc 57.143%; Test Acc 59.567%\n",
      "Epoch   860:\n",
      "Training Loss 0.194; Training Acc 96.700%; Validation Acc 56.962%; Test Acc 59.567%\n",
      "Epoch   870:\n",
      "Training Loss 0.194; Training Acc 96.700%; Validation Acc 57.324%; Test Acc 59.025%\n",
      "Epoch   880:\n",
      "Training Loss 0.194; Training Acc 96.722%; Validation Acc 58.590%; Test Acc 58.664%\n",
      "Epoch   890:\n",
      "Training Loss 0.194; Training Acc 96.745%; Validation Acc 57.324%; Test Acc 59.025%\n",
      "Epoch   900:\n",
      "Training Loss 0.194; Training Acc 96.745%; Validation Acc 56.962%; Test Acc 59.025%\n",
      "Epoch   910:\n",
      "Training Loss 0.194; Training Acc 96.722%; Validation Acc 57.143%; Test Acc 58.303%\n",
      "Epoch   920:\n",
      "Training Loss 0.194; Training Acc 96.745%; Validation Acc 56.962%; Test Acc 58.303%\n",
      "Epoch   930:\n",
      "Training Loss 0.194; Training Acc 96.768%; Validation Acc 56.600%; Test Acc 58.845%\n",
      "Epoch   940:\n",
      "Training Loss 0.194; Training Acc 96.768%; Validation Acc 56.962%; Test Acc 58.664%\n",
      "Epoch   950:\n",
      "Training Loss 0.194; Training Acc 96.768%; Validation Acc 57.143%; Test Acc 57.942%\n",
      "Epoch   960:\n",
      "Training Loss 0.194; Training Acc 96.768%; Validation Acc 57.324%; Test Acc 59.567%\n",
      "Epoch   970:\n",
      "Training Loss 0.194; Training Acc 96.768%; Validation Acc 55.877%; Test Acc 58.845%\n",
      "Epoch   980:\n",
      "Training Loss 0.194; Training Acc 96.813%; Validation Acc 55.696%; Test Acc 58.664%\n",
      "Epoch   990:\n",
      "Training Loss 0.194; Training Acc 96.813%; Validation Acc 56.781%; Test Acc 59.025%\n",
      "Epoch  1000:\n",
      "Training Loss 0.194; Training Acc 96.813%; Validation Acc 58.409%; Test Acc 58.664%\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.00001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    cm=[[0,0,0,0],[0,0,0,0],[0,0,0,0],[0,0,0,0]]\n",
    "    train_loss = 0.0\n",
    "    val_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    val_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    for batch, (inputs,targets) in enumerate(train_loader, 0):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.max(outputs, 1)[1]\n",
    "        classes = torch.max(targets, 1)[1]\n",
    "        train_correct = (preds == classes).sum()\n",
    "        train_acc += train_correct.item()\n",
    "        train_loss += loss.item()\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            for inputs,targets in val_loader:\n",
    "                outputs = net(inputs)\n",
    "                preds = torch.max(outputs, 1)[1]\n",
    "                classes = torch.max(targets, 1)[1]\n",
    "                val_correct = (preds == classes).sum()\n",
    "                val_acc += val_correct.item()\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "            for inputs,targets in test_loader:\n",
    "                outputs = net(inputs)\n",
    "                preds = torch.max(outputs, 1)[1]\n",
    "                classes = torch.max(targets, 1)[1]\n",
    "                cm=conf_mat(cm,Tlabels(classes.numpy()),Plabels(preds.numpy())) #new\n",
    "                test_correct = (preds == classes).sum()\n",
    "                test_acc += test_correct.item()\n",
    "            net.train()\n",
    "        Train_loss = train_loss/(len(train_set))\n",
    "        Train_acc = 100*train_acc/(len(train_set))\n",
    "        Val_acc = 100*val_acc/(len(val_set))\n",
    "        Test_acc = 100*test_acc/(len(test_set))\n",
    "        print(f'Epoch {epoch:5d}:')\n",
    "        print(f'Training Loss {Train_loss:.3f}; Training Acc {Train_acc:.3f}%; Validation Acc {Val_acc:.3f}%; Test Acc {Test_acc:.3f}%')   \n",
    "        cmplot(cm) #new     \n",
    "print('Finished training')\n",
    "\n",
    "# after training, choose epoch based on validation accuracy\n",
    "# to do:\n",
    "# plot the loss/accuracy\n",
    "# get confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1051c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
